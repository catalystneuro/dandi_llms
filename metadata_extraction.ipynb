{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata extraction\n",
    "\n",
    "The general context of this problem is automatic metadata extraction for neuroscience datasets. \n",
    "As a proof of concept, this notebook showcases how to extact relevant experimental metadata from the abstract using LLMs. \n",
    "Following the categories currently used in DANDI for metadata handling we focus on the following fields of metadata:\n",
    "\n",
    "- Species\n",
    "- Species identifier (UBERON ID)\n",
    "- Anatomy\n",
    "- Anatomy identifier (NCBI Taxonomy ID)\n",
    "- Approach\n",
    "- Measurement technique\n",
    "\n",
    "The approach here is  inspired by [recent research](https://ar5iv.labs.arxiv.org/html/2304.10428) in named entity recognition (NER) using LLMs.\n",
    "More specifically, the technique that we use is in-context learning, where a set of relevant examples are presented\n",
    "as context or instructions for inference to tailor a pre-trained language model to a specific task. The core of this \n",
    "technique is the selection of relevant examples to be used as context. Currently, those were created from a subset of\n",
    "well annotated DANDI datasets (see `../data/training_data.json`).\n",
    "\n",
    "Schematically, the pipeline during inference works as follows:\n",
    "- Extract a set of examples to build a context-prompt.\n",
    "- From an abstract passed as text or a doi which can be used to fetch the abstract using [Crossref API](https://www.crossref.org/documentation/retrieve-metadata/rest-api/) create a task prompt.\n",
    "- Query OpenAPI endpoint for inference using the context-prompt and task-prompt.\n",
    "- Parse the results and extract the relevant metadata.\n",
    "\n",
    "\n",
    "Structure of a context-prompt:\n",
    "\n",
    "```\n",
    "You are a neuroscience researcher and you are interested in figuring the metadata from abstracts. Here are some\n",
    "examples of how you work:\n",
    "\n",
    "Abstract from example 1\n",
    "Metadata from example 1\n",
    "\n",
    "Abstract from example 2\n",
    "Metadata from example 2\n",
    "```\n",
    "\n",
    "Structure of a task-prompt:\n",
    "\n",
    "```\n",
    "The abstract of the paper is:\n",
    "{abstract} \n",
    "\n",
    "Fill as in the examples:\n",
    "Information: {{}}\n",
    "In the format of the previous reponse. If some information is missing, leave it blank.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In context learning prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metadata_extraction import generate_zero_shot_prompt, generate_task_prompt_from_abstract, infer_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble_prompt = \"\"\"You are a neuroscience researcher and you are interested in figuring the metadata from abstracts. Here are some\n",
    "examples of how you work:\"\"\"\n",
    "\n",
    "training_dadiset_ids = [\"000568\", \"000250\", \"000147\", \"000127\", \"000055\", \"000044\"]\n",
    "zero_shot_prompt = generate_zero_shot_prompt(training_dadiset_ids)\n",
    "\n",
    "context_prompt = preamble_prompt + zero_shot_prompt\n",
    "context_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dandiset_id_test = \"000568\"\n",
    "doi = \"https://doi.org/10.1038/s41593-022-01138-x\" \n",
    "abstract_to_test = \"The incorporation of new information into the hippocampal network is likely to be constrained by its innate architecture and internally generated activity patterns. However, the origin, organization and consequences of such patterns remain poorly understood. In the present study we show that hippocampal network dynamics are affected by sequential neurogenesis. We birthdated CA1 pyramidal neurons with in utero electroporation over 4 embryonic days, encompassing the peak of hippocampal neurogenesis, and compared their functional features in freely moving adult mice. Neurons of the same birthdate displayed distinct connectivity, coactivity across brain states and assembly dynamics. Same-birthdate neurons exhibited overlapping spatial representations, which were maintained across different environments. Overall, the wiring and functional features of CA1 pyramidal neurons reflected a combination of birthdate and the rate of neurogenesis. These observations demonstrate that sequential neurogenesis during embryonic development shapes the preconfigured forms of adult network dynamics.\"\n",
    "task_prompt = generate_task_prompt_from_abstract(abstract_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = generate_task_prompt_from_abstract(abstract_to_test)\n",
    "context_prompt = preamble_prompt + zero_shot_prompt\n",
    "prompt = f\"{context_prompt} {task_prompt}\"\n",
    "\n",
    "print(\"abstract: \\n\")\n",
    "pprint.pprint(abstract_to_test)\n",
    "print(\"\\n Information extracted: \\n\")\n",
    "metadata = infer_metadata(prompt)\n",
    "pprint.pprint(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline without context learning for sanity check\n",
    "1) Same as our pipeline, but without context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = preamble_prompt \n",
    "prompt = f\"{context_prompt} {task_prompt}\"\n",
    "\n",
    "print(\"abstract: \\n\")\n",
    "pprint.pprint(abstract_to_test)\n",
    "print(\"\\n Information extracted: \\n\")\n",
    "metadata = infer_metadata(prompt)\n",
    "pprint.pprint(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better baseline\n",
    "For a comparision that is more fair, here we attempt to make the baseline better by using a more sophisticated prompt and more clear expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_prompt_from_abstract_without_context(abstract: str) -> str:\n",
    "    prompt = f\"\"\"The abstract of the paper is:\n",
    "    {abstract} \n",
    "\n",
    "    Extract the following information from the abstract:\n",
    "    - species:\n",
    "    - species identifier in the NCBI taxonomy:\n",
    "    - approach:\n",
    "    - measurement:\n",
    "    - anatomy:\n",
    "    - anatomy identifier in the Uberon ontology:\n",
    "\n",
    "    Return the response as a JSON object with the following format:\n",
    "    \n",
    "    {{\n",
    "        \"species\": [species_name_1, species_name_2, ...],\n",
    "        \"species_identifier\": [species identifiers in the NCBI taxonomy. e.g 'http://purl.obolibrary.org/obo/NCBITaxon_10090'],\n",
    "        \"approach\": [e.g. 'electrophysiology', 'calcium imaging', 'optogenetics'],\n",
    "        \"measurement\": [e.g. surgery, spike sorting, etc.],\n",
    "        \"anatomy\": [e.g. 'hippocampus', 'cortex', 'thalamus'],\n",
    "        \"anatomy_identifier\": [anatomy identifier in the Uberon ontology]\n",
    "    }}\n",
    "    \n",
    "    If some information is missing, leave it blank.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = preamble_prompt \n",
    "task_prompt = generate_task_prompt_from_abstract_without_context(abstract_to_test)\n",
    "prompt = f\"{context_prompt} {task_prompt}\"\n",
    "\n",
    "print(\"abstract: \\n\")\n",
    "pprint.pprint(abstract_to_test)\n",
    "print(\"\\n Information extracted: \\n\")\n",
    "metadata = infer_metadata(prompt)\n",
    "pprint.pprint(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DOI to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metadata_extraction import get_crossref_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random article from elife\n",
    "doi = \"https://doi.org/10.7554/eLife.89093.1\" \n",
    "abstract_to_test = get_crossref_abstract(doi)\n",
    "\n",
    "task_prompt = generate_task_prompt_from_abstract(abstract_to_test)\n",
    "context_prompt = preamble_prompt + zero_shot_prompt\n",
    "prompt = f\"{context_prompt} {task_prompt}\"\n",
    "\n",
    "print(\"abstract: \\n\")\n",
    "pprint.pprint(abstract_to_test)\n",
    "print(\"\\n Information extracted: \\n\")\n",
    "metadata = infer_metadata(prompt)\n",
    "pprint.pprint(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = preamble_prompt \n",
    "task_prompt = generate_task_prompt_from_abstract_without_context(abstract_to_test)\n",
    "prompt = f\"{context_prompt} {task_prompt}\"\n",
    "\n",
    "print(\"abstract: \\n\")\n",
    "pprint.pprint(abstract_to_test)\n",
    "print(\"\\n Information extracted: \\n\")\n",
    "metadata = infer_metadata(prompt)\n",
    "pprint.pprint(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandi_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
